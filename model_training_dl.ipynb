{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting experiments with multiple tabular models...\n",
      "Using device: cpu\n",
      "\n",
      "==================================================\n",
      "Training TabR\n",
      "==================================================\n",
      "\n",
      "TabR - Epoch 0:\n",
      "Train Loss: 0.7757\n",
      "Train Acc: 69.47%\n",
      "Test Acc: 88.43%\n",
      "Best Test Acc: 88.43%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.96      0.92       449\n",
      "         1.0       0.93      0.81      0.86       277\n",
      "\n",
      "    accuracy                           0.90       726\n",
      "   macro avg       0.91      0.88      0.89       726\n",
      "weighted avg       0.90      0.90      0.90       726\n",
      "\n",
      "\n",
      "TabR - Epoch 5:\n",
      "Train Loss: 0.2872\n",
      "Train Acc: 90.67%\n",
      "Test Acc: 89.64%\n",
      "Best Test Acc: 90.27%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.97      0.93       449\n",
      "         1.0       0.95      0.82      0.88       277\n",
      "\n",
      "    accuracy                           0.91       726\n",
      "   macro avg       0.92      0.90      0.91       726\n",
      "weighted avg       0.92      0.91      0.91       726\n",
      "\n",
      "\n",
      "TabR - Epoch 10:\n",
      "Train Loss: 0.2651\n",
      "Train Acc: 90.33%\n",
      "Test Acc: 90.28%\n",
      "Best Test Acc: 90.28%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.96      0.94       449\n",
      "         1.0       0.94      0.84      0.89       277\n",
      "\n",
      "    accuracy                           0.92       726\n",
      "   macro avg       0.92      0.90      0.91       726\n",
      "weighted avg       0.92      0.92      0.92       726\n",
      "\n",
      "\n",
      "TabR - Epoch 15:\n",
      "Train Loss: 0.2586\n",
      "Train Acc: 90.26%\n",
      "Test Acc: 89.77%\n",
      "Best Test Acc: 90.62%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.93       449\n",
      "         1.0       0.90      0.86      0.88       277\n",
      "\n",
      "    accuracy                           0.91       726\n",
      "   macro avg       0.91      0.90      0.90       726\n",
      "weighted avg       0.91      0.91      0.91       726\n",
      "\n",
      "\n",
      "TabR - Epoch 20:\n",
      "Train Loss: 0.2434\n",
      "Train Acc: 91.11%\n",
      "Test Acc: 90.37%\n",
      "Best Test Acc: 90.62%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.96      0.93       449\n",
      "         1.0       0.92      0.85      0.89       277\n",
      "\n",
      "    accuracy                           0.92       726\n",
      "   macro avg       0.92      0.90      0.91       726\n",
      "weighted avg       0.92      0.92      0.92       726\n",
      "\n",
      "\n",
      "TabR - Epoch 25:\n",
      "Train Loss: 0.2373\n",
      "Train Acc: 91.18%\n",
      "Test Acc: 90.82%\n",
      "Best Test Acc: 90.82%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.96      0.94       449\n",
      "         1.0       0.94      0.85      0.89       277\n",
      "\n",
      "    accuracy                           0.92       726\n",
      "   macro avg       0.93      0.91      0.92       726\n",
      "weighted avg       0.92      0.92      0.92       726\n",
      "\n",
      "\n",
      "TabR - Epoch 30:\n",
      "Train Loss: 0.2360\n",
      "Train Acc: 91.24%\n",
      "Test Acc: 90.96%\n",
      "Best Test Acc: 90.96%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94       449\n",
      "         1.0       0.93      0.86      0.89       277\n",
      "\n",
      "    accuracy                           0.92       726\n",
      "   macro avg       0.92      0.91      0.92       726\n",
      "weighted avg       0.92      0.92      0.92       726\n",
      "\n",
      "\n",
      "TabR - Epoch 35:\n",
      "Train Loss: 0.2357\n",
      "Train Acc: 91.35%\n",
      "Test Acc: 90.84%\n",
      "Best Test Acc: 90.96%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94       449\n",
      "         1.0       0.93      0.86      0.89       277\n",
      "\n",
      "    accuracy                           0.92       726\n",
      "   macro avg       0.92      0.91      0.91       726\n",
      "weighted avg       0.92      0.92      0.92       726\n",
      "\n",
      "\n",
      "TabR - Epoch 40:\n",
      "Train Loss: 0.2321\n",
      "Train Acc: 91.46%\n",
      "Test Acc: 90.78%\n",
      "Best Test Acc: 90.96%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94       449\n",
      "         1.0       0.93      0.86      0.89       277\n",
      "\n",
      "    accuracy                           0.92       726\n",
      "   macro avg       0.92      0.91      0.91       726\n",
      "weighted avg       0.92      0.92      0.92       726\n",
      "\n",
      "Early stopping at epoch 40\n",
      "\n",
      "==================================================\n",
      "Training FTTransformer\n",
      "==================================================\n",
      "\n",
      "FTTransformer - Epoch 0:\n",
      "Train Loss: 0.3201\n",
      "Train Acc: 86.55%\n",
      "Test Acc: 88.45%\n",
      "Best Test Acc: 88.45%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.92      0.92       449\n",
      "         1.0       0.87      0.84      0.86       277\n",
      "\n",
      "    accuracy                           0.89       726\n",
      "   macro avg       0.89      0.88      0.89       726\n",
      "weighted avg       0.89      0.89      0.89       726\n",
      "\n",
      "\n",
      "FTTransformer - Epoch 5:\n",
      "Train Loss: 0.2568\n",
      "Train Acc: 88.70%\n",
      "Test Acc: 85.18%\n",
      "Best Test Acc: 88.45%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.99      0.91       449\n",
      "         1.0       0.98      0.71      0.82       277\n",
      "\n",
      "    accuracy                           0.88       726\n",
      "   macro avg       0.91      0.85      0.87       726\n",
      "weighted avg       0.90      0.88      0.88       726\n",
      "\n",
      "\n",
      "FTTransformer - Epoch 10:\n",
      "Train Loss: 0.2285\n",
      "Train Acc: 90.09%\n",
      "Test Acc: 89.33%\n",
      "Best Test Acc: 89.33%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.96      0.93       449\n",
      "         1.0       0.93      0.83      0.87       277\n",
      "\n",
      "    accuracy                           0.91       726\n",
      "   macro avg       0.91      0.89      0.90       726\n",
      "weighted avg       0.91      0.91      0.91       726\n",
      "\n",
      "\n",
      "FTTransformer - Epoch 15:\n",
      "Train Loss: 0.2269\n",
      "Train Acc: 89.87%\n",
      "Test Acc: 88.32%\n",
      "Best Test Acc: 90.22%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.96      0.92       449\n",
      "         1.0       0.92      0.81      0.86       277\n",
      "\n",
      "    accuracy                           0.90       726\n",
      "   macro avg       0.91      0.88      0.89       726\n",
      "weighted avg       0.90      0.90      0.90       726\n",
      "\n",
      "\n",
      "FTTransformer - Epoch 20:\n",
      "Train Loss: 0.2151\n",
      "Train Acc: 90.63%\n",
      "Test Acc: 88.94%\n",
      "Best Test Acc: 90.22%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.97      0.93       449\n",
      "         1.0       0.94      0.81      0.87       277\n",
      "\n",
      "    accuracy                           0.91       726\n",
      "   macro avg       0.92      0.89      0.90       726\n",
      "weighted avg       0.91      0.91      0.91       726\n",
      "\n",
      "Early stopping at epoch 23\n",
      "\n",
      "==================================================\n",
      "Training SAINT\n",
      "==================================================\n",
      "\n",
      "SAINT - Epoch 0:\n",
      "Train Loss: 0.3289\n",
      "Train Acc: 86.76%\n",
      "Test Acc: 85.00%\n",
      "Best Test Acc: 85.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.96      0.91       449\n",
      "         1.0       0.92      0.74      0.82       277\n",
      "\n",
      "    accuracy                           0.88       726\n",
      "   macro avg       0.89      0.85      0.86       726\n",
      "weighted avg       0.88      0.88      0.87       726\n",
      "\n",
      "\n",
      "SAINT - Epoch 5:\n",
      "Train Loss: 0.2561\n",
      "Train Acc: 89.11%\n",
      "Test Acc: 86.25%\n",
      "Best Test Acc: 88.43%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.98      0.92       449\n",
      "         1.0       0.95      0.75      0.84       277\n",
      "\n",
      "    accuracy                           0.89       726\n",
      "   macro avg       0.91      0.86      0.88       726\n",
      "weighted avg       0.90      0.89      0.89       726\n",
      "\n",
      "\n",
      "SAINT - Epoch 10:\n",
      "Train Loss: 0.2379\n",
      "Train Acc: 89.22%\n",
      "Test Acc: 88.00%\n",
      "Best Test Acc: 88.43%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.96      0.92       449\n",
      "         1.0       0.93      0.80      0.86       277\n",
      "\n",
      "    accuracy                           0.90       726\n",
      "   macro avg       0.91      0.88      0.89       726\n",
      "weighted avg       0.90      0.90      0.90       726\n",
      "\n",
      "\n",
      "SAINT - Epoch 15:\n",
      "Train Loss: 0.2261\n",
      "Train Acc: 89.48%\n",
      "Test Acc: 87.81%\n",
      "Best Test Acc: 89.85%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.98      0.93       449\n",
      "         1.0       0.96      0.78      0.86       277\n",
      "\n",
      "    accuracy                           0.90       726\n",
      "   macro avg       0.92      0.88      0.89       726\n",
      "weighted avg       0.91      0.90      0.90       726\n",
      "\n",
      "\n",
      "SAINT - Epoch 20:\n",
      "Train Loss: 0.2171\n",
      "Train Acc: 89.67%\n",
      "Test Acc: 89.17%\n",
      "Best Test Acc: 89.85%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.97      0.93       449\n",
      "         1.0       0.95      0.81      0.87       277\n",
      "\n",
      "    accuracy                           0.91       726\n",
      "   macro avg       0.92      0.89      0.90       726\n",
      "weighted avg       0.91      0.91      0.91       726\n",
      "\n",
      "Early stopping at epoch 23\n",
      "\n",
      "==================================================\n",
      "Training TabNet\n",
      "==================================================\n",
      "\n",
      "TabNet - Epoch 0:\n",
      "Train Loss: 0.3999\n",
      "Train Acc: 82.71%\n",
      "Test Acc: 87.58%\n",
      "Best Test Acc: 87.58%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.98      0.92       449\n",
      "         1.0       0.95      0.78      0.85       277\n",
      "\n",
      "    accuracy                           0.90       726\n",
      "   macro avg       0.91      0.88      0.89       726\n",
      "weighted avg       0.90      0.90      0.90       726\n",
      "\n",
      "\n",
      "TabNet - Epoch 5:\n",
      "Train Loss: 0.2442\n",
      "Train Acc: 89.95%\n",
      "Test Acc: 88.35%\n",
      "Best Test Acc: 89.78%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.98      0.93       449\n",
      "         1.0       0.96      0.79      0.87       277\n",
      "\n",
      "    accuracy                           0.91       726\n",
      "   macro avg       0.92      0.88      0.90       726\n",
      "weighted avg       0.91      0.91      0.90       726\n",
      "\n",
      "\n",
      "TabNet - Epoch 10:\n",
      "Train Loss: 0.2395\n",
      "Train Acc: 89.64%\n",
      "Test Acc: 89.89%\n",
      "Best Test Acc: 89.89%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.97      0.93       449\n",
      "         1.0       0.95      0.83      0.88       277\n",
      "\n",
      "    accuracy                           0.92       726\n",
      "   macro avg       0.92      0.90      0.91       726\n",
      "weighted avg       0.92      0.92      0.91       726\n",
      "\n",
      "\n",
      "TabNet - Epoch 15:\n",
      "Train Loss: 0.2350\n",
      "Train Acc: 89.61%\n",
      "Test Acc: 89.60%\n",
      "Best Test Acc: 89.89%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.97      0.93       449\n",
      "         1.0       0.94      0.82      0.88       277\n",
      "\n",
      "    accuracy                           0.91       726\n",
      "   macro avg       0.92      0.90      0.91       726\n",
      "weighted avg       0.92      0.91      0.91       726\n",
      "\n",
      "\n",
      "TabNet - Epoch 20:\n",
      "Train Loss: 0.2204\n",
      "Train Acc: 90.52%\n",
      "Test Acc: 89.33%\n",
      "Best Test Acc: 89.89%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.96      0.93       449\n",
      "         1.0       0.93      0.83      0.87       277\n",
      "\n",
      "    accuracy                           0.91       726\n",
      "   macro avg       0.91      0.89      0.90       726\n",
      "weighted avg       0.91      0.91      0.91       726\n",
      "\n",
      "Early stopping at epoch 20\n",
      "\n",
      "==================================================\n",
      "FINAL RESULTS COMPARISON\n",
      "==================================================\n",
      "\n",
      "TabR:\n",
      "Best Test Accuracy: 90.96%\n",
      "Final Train Accuracy: 91.46%\n",
      "Final Test Accuracy: 90.78%\n",
      "\n",
      "FTTransformer:\n",
      "Best Test Accuracy: 90.22%\n",
      "Final Train Accuracy: 91.00%\n",
      "Final Test Accuracy: 88.72%\n",
      "\n",
      "SAINT:\n",
      "Best Test Accuracy: 89.85%\n",
      "Final Train Accuracy: 90.68%\n",
      "Final Test Accuracy: 89.40%\n",
      "\n",
      "TabNet:\n",
      "Best Test Accuracy: 89.89%\n",
      "Final Train Accuracy: 90.52%\n",
      "Final Test Accuracy: 89.33%\n",
      "\n",
      "Results saved successfully:\n",
      "- Model checkpoints: tabular_models_results/*_best.pth\n",
      "- Confusion matrices: tabular_models_results/*_confusion_matrix.png\n",
      "- Results comparison: tabular_models_results/model_comparison.png\n",
      "- Detailed results: tabular_models_results/results.json\n",
      "- Training parameters: tabular_models_results/training_params.json\n",
      "- Summary: tabular_models_results/summary.txt\n",
      "\n",
      "Experiments completed. Results saved in 'tabular_models_results' directory.\n",
      "Check the results.json and training_params.json files for detailed information.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "class StudentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class TabR(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_hidden=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden)\n",
    "        ])\n",
    "        self.replicate = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, input_size) for _ in range(num_hidden)\n",
    "        ])\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size//2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.input_proj(x))\n",
    "        replication_loss = 0\n",
    "        for i in range(self.num_hidden):\n",
    "            h_prev = h\n",
    "            h = self.activation(self.hidden_layers[i](h))\n",
    "            h = self.dropout(h)\n",
    "            x_pred = self.replicate[i](h_prev)\n",
    "            replication_loss += nn.MSELoss()(x_pred, x)\n",
    "        return self.output(h), replication_loss\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, input_size, d_model=128, nhead=8, num_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.feature_embeddings = nn.Linear(input_size, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_embeddings(x).unsqueeze(1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class SAINT(nn.Module):\n",
    "    def __init__(self, input_size, d_model=128, nhead=8, num_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.feature_embeddings = nn.Linear(input_size, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, input_size, d_model))\n",
    "        \n",
    "        self.intersample_attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=d_model * 4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_embeddings(x).unsqueeze(1)\n",
    "        x = x + self.pos_embedding\n",
    "        \n",
    "        x_inter, _ = self.intersample_attention(x, x, x)\n",
    "        x = x + x_inter\n",
    "        \n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class TabNet(nn.Module):\n",
    "    def __init__(self, input_size, n_d=64, n_steps=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_d = n_d\n",
    "        self.n_steps = n_steps\n",
    "        \n",
    "        self.feature_transforms = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, 2 * n_d),\n",
    "                nn.BatchNorm1d(2 * n_d),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ) for _ in range(n_steps)\n",
    "        ])\n",
    "        \n",
    "        self.attention = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(2 * n_d, input_size),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(n_steps)\n",
    "        ])\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(n_d * n_steps, n_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_d, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        features = []\n",
    "        \n",
    "        for step in range(self.n_steps):\n",
    "            transformed = self.feature_transforms[step](x)\n",
    "            attention = self.attention[step](transformed)\n",
    "            masked_x = x * attention\n",
    "            features.append(transformed[:, :self.n_d])\n",
    "        \n",
    "        combined_features = torch.cat(features, dim=1)\n",
    "        output = self.output(combined_features)\n",
    "        return output\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, \n",
    "                epochs, device, model_name, save_dir, early_stopping_patience=10):\n",
    "    model = model.to(device)\n",
    "    best_test_acc = 0\n",
    "    no_improve = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_acc': [], 'best_acc': 0}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_true = [], []\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if isinstance(model, TabR):\n",
    "                outputs, replication_loss = model(batch_X)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, batch_y) + 0.1 * replication_loss\n",
    "            else:\n",
    "                outputs = model(batch_X).squeeze()\n",
    "                loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            predicted = (outputs > 0.5).float()\n",
    "            train_preds.extend(predicted.cpu().numpy())\n",
    "            train_true.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_preds, test_true = [], []\n",
    "        test_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                if isinstance(model, TabR):\n",
    "                    outputs, _ = model(batch_X)\n",
    "                else:\n",
    "                    outputs = model(batch_X)\n",
    "                \n",
    "                outputs = outputs.squeeze()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                test_preds.extend(predicted.cpu().numpy())\n",
    "                test_true.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        train_acc = balanced_accuracy_score(train_true, train_preds)\n",
    "        test_acc = balanced_accuracy_score(test_true, test_preds)\n",
    "        \n",
    "        history['train_loss'].append(train_loss/len(train_loader))\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(test_acc)\n",
    "        \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            history['best_acc'] = test_acc\n",
    "            no_improve = 0\n",
    "            \n",
    "            # Save best model and create confusion matrix\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            \n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'test_acc': test_acc,\n",
    "                'epoch': epoch,\n",
    "            }, f'{save_dir}/{model_name}_best.pth')\n",
    "            \n",
    "            cm = confusion_matrix(test_true, test_preds)\n",
    "            plt.figure(figsize=(10,8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'{model_name} Confusion Matrix (Best Model)')\n",
    "            plt.savefig(f'{save_dir}/{model_name}_confusion_matrix.png')\n",
    "            plt.close()\n",
    "            \n",
    "        else:\n",
    "            no_improve += 1\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f'\\n{model_name} - Epoch {epoch}:')\n",
    "            print(f'Train Loss: {train_loss/len(train_loader):.4f}')\n",
    "            print(f'Train Acc: {100 * train_acc:.2f}%')\n",
    "            print(f'Test Acc: {100 * test_acc:.2f}%')\n",
    "            print(f'Best Test Acc: {100 * best_test_acc:.2f}%')\n",
    "            \n",
    "            # Print classification report\n",
    "            print('\\nClassification Report:')\n",
    "            print(classification_report(test_true, test_preds))\n",
    "        \n",
    "        if no_improve >= early_stopping_patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    return history\n",
    "\n",
    "def run_experiments():\n",
    "    # Create save directory\n",
    "    save_dir = 'tabular_models_results'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv(r'C:\\Users\\andre\\OneDrive\\Desktop\\CS485-Final-Project\\dataset\\dataset_pruned.csv')\n",
    "    X = df.drop('Target_encoded', axis=1)\n",
    "    y = df['Target_encoded']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = StudentDataset(X_train, y_train.values)\n",
    "    test_dataset = StudentDataset(X_test, y_test.values)\n",
    "    \n",
    "    # Training parameters\n",
    "    params = {\n",
    "        'batch_size': 64,\n",
    "        'epochs': 100,\n",
    "        'learning_rate': 0.001,\n",
    "    }\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'])\n",
    "    \n",
    "    # Models to test\n",
    "    models = {\n",
    "        'TabR': TabR(input_size=X_train.shape[1]),\n",
    "        'FTTransformer': FTTransformer(input_size=X_train.shape[1]),\n",
    "        'SAINT': SAINT(input_size=X_train.shape[1]),\n",
    "        'TabNet': TabNet(input_size=X_train.shape[1])\n",
    "    }\n",
    "    \n",
    "    # Results storage\n",
    "    results = {}\n",
    "    \n",
    "    # Train each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', patience=5, factor=0.5\n",
    "        )\n",
    "        \n",
    "        history = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            epochs=params['epochs'],\n",
    "            device=device,\n",
    "            model_name=model_name,\n",
    "            save_dir=save_dir\n",
    "        )\n",
    "        \n",
    "        results[model_name] = history\n",
    "    \n",
    "    # Print final comparative results\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"FINAL RESULTS COMPARISON\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for model_name, history in results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"Best Test Accuracy: {history['best_acc']*100:.2f}%\")\n",
    "        print(f\"Final Train Accuracy: {history['train_acc'][-1]*100:.2f}%\")\n",
    "        print(f\"Final Test Accuracy: {history['test_acc'][-1]*100:.2f}%\")\n",
    "    \n",
    "    # Plot comparative results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    for model_name, history in results.items():\n",
    "        plt.plot(history['test_acc'], label=f'{model_name}')\n",
    "    plt.title('Test Accuracy Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for model_name, history in results.items():\n",
    "        plt.plot(history['train_loss'], label=f'{model_name}')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save results to JSON\n",
    "    results_json = {}\n",
    "    for model_name, history in results.items():\n",
    "        results_json[model_name] = {\n",
    "            'best_accuracy': float(history['best_acc']),\n",
    "            'final_train_accuracy': float(history['train_acc'][-1]),\n",
    "            'final_test_accuracy': float(history['test_acc'][-1]),\n",
    "            'training_history': {\n",
    "                'train_loss': [float(x) for x in history['train_loss']],\n",
    "                'train_acc': [float(x) for x in history['train_acc']],\n",
    "                'test_acc': [float(x) for x in history['test_acc']]\n",
    "            }\n",
    "        }   \n",
    "    \n",
    "    # Save results to JSON file\n",
    "    with open(f'{save_dir}/results.json', 'w') as f:\n",
    "        json.dump(results_json, f, indent=4)\n",
    "    \n",
    "    # Save parameters used\n",
    "    params_used = {\n",
    "        'batch_size': params['batch_size'],\n",
    "        'epochs': params['epochs'],\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'early_stopping_patience': 10,\n",
    "        'model_configurations': {\n",
    "            'TabR': {'hidden_size': 256, 'num_hidden': 3, 'dropout': 0.1},\n",
    "            'FTTransformer': {'d_model': 256, 'nhead': 8, 'num_layers': 3, 'dropout': 0.1},\n",
    "            'SAINT': {'d_model': 128, 'nhead': 8, 'num_layers': 3, 'dropout': 0.1},\n",
    "            'TabNet': {'n_d': 64, 'n_steps': 3, 'dropout': 0.1}\n",
    "        },\n",
    "        'optimizer': 'Adam',\n",
    "        'scheduler': 'ReduceLROnPlateau',\n",
    "        'scheduler_params': {\n",
    "            'mode': 'max',\n",
    "            'patience': 5,\n",
    "            'factor': 0.5\n",
    "        },\n",
    "        'loss_function': 'BCELoss'\n",
    "    }\n",
    "    \n",
    "    with open(f'{save_dir}/training_params.json', 'w') as f:\n",
    "        json.dump(params_used, f, indent=4)\n",
    "    \n",
    "    # Create a summary text file\n",
    "    with open(f'{save_dir}/summary.txt', 'w') as f:\n",
    "        f.write(\"=== Model Performance Summary ===\\n\\n\")\n",
    "        for model_name, history in results.items():\n",
    "            f.write(f\"\\n{model_name}:\\n\")\n",
    "            f.write(f\"Best Test Accuracy: {history['best_acc']*100:.2f}%\\n\")\n",
    "            f.write(f\"Final Train Accuracy: {history['train_acc'][-1]*100:.2f}%\\n\")\n",
    "            f.write(f\"Final Test Accuracy: {history['test_acc'][-1]*100:.2f}%\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "    \n",
    "    print(\"\\nResults saved successfully:\")\n",
    "    print(f\"- Model checkpoints: {save_dir}/*_best.pth\")\n",
    "    print(f\"- Confusion matrices: {save_dir}/*_confusion_matrix.png\")\n",
    "    print(f\"- Results comparison: {save_dir}/model_comparison.png\")\n",
    "    print(f\"- Detailed results: {save_dir}/results.json\")\n",
    "    print(f\"- Training parameters: {save_dir}/training_params.json\")\n",
    "    print(f\"- Summary: {save_dir}/summary.txt\")\n",
    "    \n",
    "    return results, params_used\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nStarting experiments with multiple tabular models...\")\n",
    "    results, params = run_experiments()\n",
    "    print(\"\\nExperiments completed. Results saved in 'tabular_models_results' directory.\")\n",
    "    print(\"Check the results.json and training_params.json files for detailed information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
